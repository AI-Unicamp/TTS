{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5283fc54",
   "metadata": {},
   "source": [
    "# Installs and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55d420e",
   "metadata": {},
   "source": [
    "## Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18e59ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import IPython\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce129a5",
   "metadata": {},
   "source": [
    "## Vocoder Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f4fa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install parallel_wavegan\n",
    "!pip install h5py=='3.6.0'\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ca878e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parallel_wavegan.utils import load_model\n",
    "from parallel_wavegan.utils import read_hdf5\n",
    "from parallel_wavegan.bin.preprocess import logmelfilterbank\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2a550f",
   "metadata": {},
   "source": [
    "## TTS Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78531510",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/workspace/coqui-tts')\n",
    "from TTS.config import load_config, register_config\n",
    "from TTS.tts.models import setup_model\n",
    "from TTS.tts.models.forward_tts import ForwardTTS\n",
    "from TTS.tts.models.styleforward_tts import StyleforwardTTS\n",
    "from TTS.utils.audio import AudioProcessor\n",
    "from TTS.tts.utils.synthesis import synthesis\n",
    "from TTS.tts.utils.speakers import SpeakerManager\n",
    "from TTS.tts.utils.styles import StyleManager\n",
    "from TTS.tts.utils.visual import plot_spectrogram\n",
    "from TTS.trainer import Trainer, TrainingArgs\n",
    "from TTS.tts.datasets import load_tts_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf26d2f",
   "metadata": {},
   "source": [
    "## UMAP Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9429dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install umap-learn\n",
    "import umap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d0ae8e",
   "metadata": {},
   "source": [
    "# Load Model and Set Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779c55c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# MODEL AND DEVICE SELECTION\n",
    "run_name = \"re+class\"           \n",
    "run_select = \"last_checkpoint\"\n",
    "device = \"cpu\"\n",
    "synthesizer = \"hifi-gan\" # hifi-gan or griffin-limm\n",
    "use_cuda = True if device == \"cuda\" else False\n",
    "\n",
    "# CHECKPOINTS\n",
    "checkpoints_dict = {\n",
    "                   # Neutral Models (Speaker Look-Up)\n",
    "                   \"vctk\":{\"last_checkpoint\":\"model_file.pth.tar\"},\n",
    "                   \"neutral\":{\"last_checkpoint\":\"checkpoint_1080000.pth.tar\", \"part\":\"neutral\"},\n",
    "    \n",
    "                   # Style Finetunings (Speaker Look-Up)\n",
    "                   \"amused\":{\"last_checkpoint\":\"checkpoint_1113000.pth.tar\", \"part\":\"amused\"},\n",
    "                   \"angry\":{\"last_checkpoint\":\"checkpoint_1121000.pth.tar\", \"part\":\"angry\"},\n",
    "                   \"disgusted\":{\"last_checkpoint\":\"checkpoint_1097000.pth.tar\", \"part\":\"disgusted\"},\n",
    "                   \"sleepy\":{\"last_checkpoint\":\"checkpoint_1036000.pth.tar\", \"part\":\"Sleepy\"},\n",
    "    \n",
    "                   # Speaker Finetunings (Style Look-Up)\n",
    "                   \"sam\":{\"last_checkpoint\": \"checkpoint_1390000.pth.tar\", \"part\":\"sam\"},\n",
    "                   \"josh\":{\"last_checkpoint\": \"checkpoint_1150000.pth.tar\", \"part\":\"josh\"},\n",
    "                   \"jenie\":{\"last_checkpoint\": \"checkpoint_1160000.pth.tar\", \"part\":\"jenie\"},\n",
    "                   \"bea\":{\"last_checkpoint\": \"checkpoint_1130000.pth.tar\", \"part\":\"bea\"},\n",
    "    \n",
    "                   # Multi-Speaker Multi-Style (Double Look-Up)\n",
    "                   \"double_lookup\":{\"last_checkpoint\": \"checkpoint_1240000.pth.tar\", \"part\":\"all\"},\n",
    "    \n",
    "                   # Representation Learning\n",
    "                   \"re\":{\"last_checkpoint\": \"checkpoint_1240000.pth.tar\", \"part\":\"all\"},\n",
    "                   \"re+class\":{\"last_checkpoint\": \"checkpoint_1240000.pth.tar\", \"part\":\"all\"},\n",
    "                    }\n",
    "\n",
    "# EXPERIMENT FOLDER\n",
    "folder = \"../experiments/\" + run_name + \"/\"\n",
    "\n",
    "# LOAD CONFIG\n",
    "config = load_config(folder + \"config.json\")\n",
    "\n",
    "# LOAD SPEAKERS\n",
    "if os.path.isfile(folder + \"speakers.json\"):\n",
    "    spk_file_path = folder + \"speakers.json\"\n",
    "    spk_manager = SpeakerManager(speaker_id_file_path = spk_file_path)\n",
    "    with open(spk_file_path) as json_file:\n",
    "        spk_to_id = json.load(json_file)\n",
    "else:\n",
    "    spk_to_id = {}\n",
    "    spk_manager = None\n",
    "    \n",
    "# LOAD STYLES\n",
    "if os.path.isfile(folder + \"style_ids.json\"):\n",
    "    sty_file_path = folder + \"style_ids.json\"\n",
    "    sty_manager = StyleManager(style_ids_file_path = sty_file_path)\n",
    "    with open(sty_file_path) as json_file:\n",
    "        sty_to_id = json.load(json_file)\n",
    "else:\n",
    "    sty_to_id = {}\n",
    "    sty_manager = None\n",
    "    \n",
    "# LOAD MODEL        \n",
    "model = setup_model(config, speaker_manager = spk_manager, style_manager = sty_manager)\n",
    "\n",
    "# LOAD CHECKPOINT\n",
    "checkpoint = torch.load(folder + checkpoints_dict[run_name][run_select], map_location=torch.device(device))['model']\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# PREPARE MODEL\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# PREPARE VOCODER\n",
    "if synthesizer != \"griffin-limm\":\n",
    "    voc_name = synthesizer\n",
    "    voc_checkpoint = \"../experiments/vocoders/\" + voc_name + \"/checkpoint-470000steps.pkl\"\n",
    "    voc_config_path = \"../experiments/vocoders/\" + voc_name + \"/config.yml\"\n",
    "    voc_stats = \"../experiments/vocoders/\" + voc_name + \"/stats.h5\"\n",
    "    with open(voc_config_path) as f:\n",
    "        voc_config = yaml.load(f, Loader = yaml.Loader)\n",
    "    vocoder = load_model(voc_checkpoint, voc_config)\n",
    "    vocoder.to(device)\n",
    "    vocoder.eval()\n",
    "    vocoder.remove_weight_norm()\n",
    "    \n",
    "    # CHECK COMPATIBILITY WITH TTS\n",
    "    config.audio.log_func = 'np.log10'\n",
    "    assert voc_config['sampling_rate'] == config.audio.sample_rate\n",
    "    assert voc_config['fmax'] == config.audio.mel_fmax\n",
    "    assert voc_config['fmin'] == config.audio.mel_fmin\n",
    "    assert voc_config['fft_size'] == config.audio.fft_size\n",
    "    assert voc_config['hop_size'] == config.audio.hop_length\n",
    "    assert voc_config['win_length'] == config.audio.win_length\n",
    "else:\n",
    "    pass\n",
    "\n",
    "\n",
    "# Audio Processor\n",
    "ap = AudioProcessor(**config.audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962e78b9",
   "metadata": {},
   "source": [
    "# Audios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b221466",
   "metadata": {},
   "source": [
    "Outputs audios of the synthesis, ressynthesis and ground-truths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b2e16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_emovdb_metadata(dataset, sub):\n",
    "    # Get csv\n",
    "    df = pd.read_csv(\"../recipes/emovdb/emovdb/\" + \"metadata/metadata_\"+ dataset + \"_\" + sub + \".csv\", sep = \"\\n\", header=None)\n",
    "    lines = [item for sublist in df.values.tolist() for item in sublist]\n",
    "    # Parse csv\n",
    "    file_names = []\n",
    "    texts = []\n",
    "    spks = []\n",
    "    styles = []\n",
    "    for line in lines:\n",
    "        file_names.append('../recipes/emovdb/emovdb/files/'+ line.split(sep='|')[0])\n",
    "        texts.append(line.split(sep='|')[1])\n",
    "        spks.append(line.split(sep='|')[2])\n",
    "        styles.append(line.split(sep='|')[3])\n",
    "    return {'texts':texts, 'speakers':spks, 'styles':styles, 'style_wavs':file_names}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fd9a9e",
   "metadata": {},
   "source": [
    "## Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd72c31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fetch Inputs\n",
    "dataset = \"test\"\n",
    "partition = checkpoints_dict[run_name][\"part\"]\n",
    "data = read_emovdb_metadata(dataset,partition)\n",
    "idx = 0\n",
    "\n",
    "# Or Insert directly\n",
    "text = data['texts'][idx]\n",
    "speaker = data['speakers'][idx]\n",
    "style = data['styles'][idx]\n",
    "style_wav = data['style_wavs'][idx]\n",
    "style_representation = style_representation if style_representation else None\n",
    "\n",
    "# SYNTHESIS\n",
    "if synthesizer == \"griffin-limm\":\n",
    "    out = synthesis(use_griffin_lim=True, text = text, speaker_id = spk_to_id.get(speaker), style_id = sty_to_id.get(style), style_wav = style_wav, style_representation = style_representation, model = model, CONFIG = config, use_cuda = use_cuda, ap = ap)\n",
    "else:\n",
    "    fp_out = synthesis(text = text, speaker_id = spk_to_id.get(speaker), style_id = sty_to_id.get(style), style_wav = style_wav, style_representation = style_representation, model = model, CONFIG = config, use_cuda = use_cuda, ap = ap)\n",
    "    feat_gen_denorm = torch.Tensor(fp_out['outputs']['model_outputs'].cpu().numpy()[0]).cuda()\n",
    "    feat_gen_denorm = torch.log10(torch.exp(feat_gen_denorm))\n",
    "    feat_gen_norm = (feat_gen_denorm.cpu() - torch.from_numpy(read_hdf5(voc_stats, \"mean\"))) / torch.from_numpy(read_hdf5(voc_stats, \"scale\"))\n",
    "    out = {'wav':vocoder.inference(feat_gen_norm).cpu().detach().numpy().squeeze(1)}\n",
    "\n",
    "# RESULTS\n",
    "audio_syn = out['wav']\n",
    "print(\"Text = {}\".format(text))\n",
    "print(\"Spk = {}\".format(speaker))\n",
    "print(\"Sty = {}\".format(style))\n",
    "print(\"Sty_Wav = {}\".format(style_wav))\n",
    "print(\"Sty_Rep = {}\".format(style_representation))\n",
    "IPython.display.Audio(audio_syn, rate=config.audio.sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefb1fe7",
   "metadata": {},
   "source": [
    "## GT Resynthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf33f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_wav = ap.load_wav(data['style_wavs'][idx])\n",
    "gt_spectrogram = ap.melspectrogram(gt_wav)\n",
    "\n",
    "if synthesizer == 'griffin-limm':\n",
    "    res_wav = ap.inv_melspectrogram(gt_spectrogram)\n",
    "else:\n",
    "    feat_gen_denorm = torch.Tensor(gt_spectrogram.T).cuda()\n",
    "    feat_gen_norm = (feat_gen_denorm.cpu() - torch.from_numpy(read_hdf5(voc_stats, \"mean\"))) / torch.from_numpy(read_hdf5(voc_stats, \"scale\"))\n",
    "    res_wav = vocoder.inference(feat_gen_norm).cpu().detach().numpy().squeeze(1)\n",
    "res_spectrogram = ap.melspectrogram(res_wav)\n",
    "\n",
    "# PLAY GT+RES\n",
    "IPython.display.Audio(res_wav, rate=config.audio.sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403f0735",
   "metadata": {},
   "source": [
    "## GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2b298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(gt_wav, rate=config.audio.sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fb2855",
   "metadata": {},
   "source": [
    "# Mel-Spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb20500",
   "metadata": {},
   "source": [
    "Outputs images of mel-spectrograms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3957c2d",
   "metadata": {},
   "source": [
    "## Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e38bd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT GT MEL-SPECTROGRAM\n",
    "plot_spectrogram(ap.melspectrogram(audio_syn).T, ap, fig_size=(8,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeccf97",
   "metadata": {},
   "source": [
    "## GT Resynthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ee2185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT GT+RES MEL-SPECTROGRAM\n",
    "plot_spectrogram(res_spectrogram.T, ap, fig_size=(8,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5a24b1",
   "metadata": {},
   "source": [
    "## GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042f8be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT GT MEL-SPECTROGRAM\n",
    "plot_spectrogram(gt_spectrogram.T, ap, fig_size=(8,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d1e5ff",
   "metadata": {},
   "source": [
    "# UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44810ba8",
   "metadata": {},
   "source": [
    "Outputs style spaces and style representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c4e6c0",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24dc216",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FUNCTIONS ####\n",
    "\n",
    "def numpy_to_torch(np_array, dtype, cuda=False):\n",
    "    if np_array is None:\n",
    "        return None\n",
    "    tensor = torch.as_tensor(np_array, dtype=dtype)\n",
    "    if cuda:\n",
    "        return tensor.cuda()\n",
    "    return tensor\n",
    "\n",
    "def id_to_torch(speaker_id, cuda=False):\n",
    "    if speaker_id is not None:\n",
    "        speaker_id = np.asarray(speaker_id)\n",
    "        speaker_id = torch.from_numpy(speaker_id).unsqueeze(0)\n",
    "    if cuda:\n",
    "        return speaker_id.cuda().type(torch.long)\n",
    "    return speaker_id.type(torch.long)\n",
    "\n",
    "\n",
    "def compute_style_mel(style_wav, ap, cuda=False):\n",
    "    style_mel = torch.FloatTensor(ap.melspectrogram(\n",
    "        ap.load_wav(style_wav, sr=ap.sample_rate))).unsqueeze(0)\n",
    "    if cuda:\n",
    "        return style_mel.cuda()\n",
    "    return style_mel\n",
    "\n",
    "\n",
    "#### DATASET DEFINITIONS ####\n",
    "\n",
    "config.datasets[0].path = '../recipes/emovdb/emovdb'\n",
    "test = '../recipes/emovdb/emovdb/metadata/metadata_test_all.csv'\n",
    "test = pd.read_csv(test, delimiter='|', encoding= 'utf-8', header=None, names = ['wav_path', 'text', 'speaker', 'style']) \n",
    "test.head()\n",
    "\n",
    "# dict style2id\n",
    "map_style = {\n",
    "    'Amused': 0,\n",
    "    'Angry': 1,\n",
    "    'Disgusted': 2,\n",
    "    'Neutral': 3,\n",
    "    'Sleepy': 4\n",
    "}\n",
    "\n",
    "# dict style2id\n",
    "map_id2style = {\n",
    "    0:'Amused',\n",
    "    1:'Angry',\n",
    "    2:'Disgusted',\n",
    "    3:'Neutral',\n",
    "    4:'Sleepy' \n",
    "}\n",
    "\n",
    "def map_wavpath2style(wav_path):\n",
    "    if('Amused' in wav_path):\n",
    "        return 'Amused'\n",
    "    elif('Angry' in wav_path):\n",
    "        return 'Angry'\n",
    "    elif('Disgusted' in wav_path):\n",
    "        return 'Disgusted'\n",
    "    elif('Neutral' in wav_path):\n",
    "        return 'Neutral'\n",
    "    elif('Sleepy' in wav_path):\n",
    "        return 'Sleepy'\n",
    "    else:\n",
    "        return 'none'\n",
    "\n",
    "\n",
    "#### TRAINER ####\n",
    "\n",
    "# init trainer args\n",
    "train_args = TrainingArgs()\n",
    "\n",
    "# load training samples\n",
    "train_samples, eval_samples = load_tts_samples(config.datasets, eval_split=True)\n",
    "\n",
    "# init speaker manager\n",
    "if config.use_speaker_embedding:\n",
    "    speaker_manager = SpeakerManager(data_items=train_samples + eval_samples)\n",
    "elif config.use_d_vector_file:\n",
    "    speaker_manager = SpeakerManager(d_vectors_file_path=config.d_vector_file)\n",
    "else:\n",
    "    speaker_manager = None\n",
    "\n",
    "# init style manager\n",
    "if config.style_encoder_config.use_supervised_style:\n",
    "    style_manager = StyleManager(data_items=train_samples + eval_samples)\n",
    "    if hasattr(config, \"model_args\"):\n",
    "        config.model_args.num_styles = style_manager.num_styles\n",
    "    else:\n",
    "        config.num_styles = style_manager.num_styles\n",
    "else:\n",
    "    style_manager = None\n",
    "    \n",
    "# init the model from config\n",
    "language_manager = None\n",
    "model = setup_model(config, speaker_manager, language_manager, style_manager)\n",
    "\n",
    "# init the trainer\n",
    "trainer = Trainer(\n",
    "    train_args,\n",
    "    config,\n",
    "    config.output_path,\n",
    "    model=model,\n",
    "    train_samples=train_samples,\n",
    "    eval_samples=eval_samples,\n",
    "    training_assets={\"audio_processor\": ap},\n",
    "    parse_command_line_args=False,\n",
    ")\n",
    "\n",
    "# restore checkpoint\n",
    "checkpoint = folder + checkpoints_dict[run_name]['last_checkpoint']\n",
    "trainer.model, opt, scaler, restore_step = trainer.restore_model(config, checkpoint, trainer.model, trainer.optimizer, trainer.scaler)\n",
    "\n",
    "# extract representations\n",
    "use_cuda = True\n",
    "\n",
    "N = config['style_encoder_config']['style_embedding_dim']\n",
    "\n",
    "train_feats = np.zeros((len(test), N))\n",
    "valid_feats = np.zeros((len(eval_samples), N))\n",
    "\n",
    "styles = []\n",
    "\n",
    "for i in tqdm(range(len(test))):\n",
    "    style_wav = '../recipes/emovdb/emovdb/files/' + test.wav_path.values[i]\n",
    "    style_mel = compute_style_mel(style_wav, ap, cuda=True)\n",
    "    style_mel = numpy_to_torch(style_mel, torch.float, cuda=use_cuda)[0].T\n",
    "\n",
    "    o_en, outputs = model.cuda().style_encoder_layer.forward([torch.rand(1,384,1).cuda(),style_mel.unsqueeze(0)], None)\n",
    "\n",
    "    if(config['style_encoder_config']['se_type'] == 'vae'):\n",
    "        outputs = outputs['z']\n",
    "    elif(config['style_encoder_config']['se_type'] == 'diffusion'):\n",
    "        outputs = outputs['style']\n",
    "\n",
    "    train_feats[i] = outputs.squeeze(0).squeeze(0).detach().cpu().numpy()\n",
    "    styles.append(map_style[map_wavpath2style(test.wav_path.values[i])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ac2555",
   "metadata": {},
   "source": [
    "## Style Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb3899b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIT / TRANSFORM UMAP\n",
    "\n",
    "#u = umap.UMAP(random_state = 42)\n",
    "\n",
    "#embeddings = u.fit_transform(train_feats)\n",
    "embeddings = u.transform(train_feats) # when the umap is already trained\n",
    "\n",
    "# Creating dataframe to better manipulate\n",
    "df = pd.DataFrame({'style': styles, 'dim1': embeddings[:,0], 'dim2': embeddings[:,1]})\n",
    "\n",
    "# Plot\n",
    "df['style_id'] = df['style'].map(map_style)\n",
    "df.head(), df.tail()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,5))\n",
    "for i in range(5):\n",
    "    df_filt = df[df['style'] == i]\n",
    "    plt.scatter(df_filt['dim1'], df_filt['dim2'], label = map_id2style[i])\n",
    "plt.legend(fontsize=15)\n",
    "plt.xlabel('UMAP dim 0', fontsize = 20)\n",
    "plt.ylabel('UMAP dim 1', fontsize = 20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303fcdac",
   "metadata": {},
   "source": [
    "## Extract Centroid Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "91ed4f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_centroid = \"Amused\"\n",
    "idxs = df[df['style']== map_style[style_centroid]].index.tolist()\n",
    "style_representations = train_feats[idxs]\n",
    "centroid = style_representations.mean(axis=0)\n",
    "print(u.transform(centroid.reshape(1,-1)))\n",
    "style_representation = torch.Tensor(centroid).unsqueeze(0).to('cuda:0')\n",
    "# HOW TO GET RANDOM REPRESENTATION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
