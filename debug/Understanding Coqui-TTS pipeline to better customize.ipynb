{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\site-packages\\numpy\\.libs\\libopenblas.TXA6YQSD3GCQQC22GEQ54J2UDCXDXHWN.gfortran-win_amd64.dll\n",
      "C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n",
      "C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\site-packages\\torchaudio\\extension\\extension.py:14: UserWarning: torchaudio C++ extension is not available.\n",
      "  warnings.warn('torchaudio C++ extension is not available.')\n",
      "C:\\Users\\lucas\\Anaconda3\\envs\\m_audio\\lib\\site-packages\\torchaudio\\backend\\utils.py:64: UserWarning: The interface of \"soundfile\" backend is planned to change in 0.8.0 to match that of \"sox_io\" backend and the current interface will be removed in 0.9.0. To use the new interface, do `torchaudio.USE_SOUNDFILE_LEGACY_INTERFACE = False` before setting the backend to \"soundfile\". Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  'The interface of \"soundfile\" backend is planned to change in 0.8.0 to '\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from TTS.config.shared_configs import BaseAudioConfig\n",
    "from TTS.tts.configs.shared_configs import BaseDatasetConfig, GSTConfig\n",
    "from TTS.tts.datasets import load_tts_samples\n",
    "from TTS.utils.audio import AudioProcessor\n",
    "from TTS.trainer_windows import Trainer, TrainingArgs\n",
    "\n",
    "# Old Tacotron Imports\n",
    "from TTS.tts.models.tacotron2 import Tacotron2\n",
    "from TTS.tts.configs.tacotron2_config import Tacotron2Config\n",
    "from TTS.tts.configs.shared_configs import GSTConfig\n",
    "\n",
    "# Style Tacotron Imports\n",
    "from TTS.tts.models.styletacotron2 import Styletacotron2\n",
    "from TTS.tts.configs.styletacotron2_config import Styletacotron2Config\n",
    "from TTS.style_encoder.configs.style_encoder_config import StyleEncoderConfig\n",
    "\n",
    "# Style forward TTS Imports\n",
    "from TTS.tts.models.styleforward_tts import StyleForwardTTS\n",
    "from TTS.tts.configs.stylefast_pitch_config import StyleFastPitchConfig\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:60.0\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:2.718281828459045\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " | > Found 13100 files in D:\\Mestrado\\Emotion Audio Synthesis (TTS)\\repo_final\\pt_etts\\data\\LJSpeech\\LJSpeech-1.1\n"
     ]
    }
   ],
   "source": [
    "seed_everything(42)\n",
    "output_path = './'\n",
    "\n",
    "# init configs\n",
    "dataset_config = BaseDatasetConfig(\n",
    "    name=\"ljspeech\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"D:/Mestrado/Emotion Audio Synthesis (TTS)/repo_final/pt_etts/data/LJSpeech\\LJSpeech-1.1\")\n",
    ")\n",
    "\n",
    "audio_config = BaseAudioConfig(\n",
    "    sample_rate=22050,\n",
    "    do_trim_silence=True,\n",
    "    trim_db=60.0,\n",
    "    signal_norm=False,\n",
    "    mel_fmin=0.0,\n",
    "    mel_fmax=8000,\n",
    "    spec_gain=1.0,\n",
    "    log_func=\"np.log\",\n",
    "    ref_level_db=20,\n",
    "    preemphasis=0.0,\n",
    ")\n",
    "\n",
    "style_config = StyleEncoderConfig(se_type=\"vaeflow\",\n",
    "                                 use_proj_linear = True,\n",
    "                                 use_nonlinear_proj = True,\n",
    "                                 proj_dim = 384,\n",
    "                                 agg_type = 'sum',\n",
    "                                 start_loss_at = 500)\n",
    "\n",
    "config = StyleFastPitchConfig(  # This is the config that is saved for the future use\n",
    "    style_encoder_config = style_config,\n",
    "    audio=audio_config,\n",
    "    batch_size=64,\n",
    "    eval_batch_size=16,\n",
    "    num_loader_workers=4,\n",
    "    num_eval_loader_workers=4,\n",
    "    run_eval=True,\n",
    "    test_delay_epochs=-1,\n",
    "    r=6,\n",
    "#     gradual_training=[[0, 6, 64], [10000, 4, 32], [50000, 3, 32], [100000, 2, 32]],\n",
    "#     double_decoder_consistency=True,\n",
    "    epochs=1000,\n",
    "    text_cleaner=\"phoneme_cleaners\",\n",
    "    use_phonemes=True,\n",
    "    phoneme_language=\"en-us\",\n",
    "    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n",
    "    print_step=25,\n",
    "    print_eval=True,\n",
    "    mixed_precision=False,\n",
    "    output_path=output_path,\n",
    "    datasets=[dataset_config],\n",
    ")\n",
    "# init audio processor\n",
    "ap = AudioProcessor(**config.audio.to_dict())\n",
    "\n",
    "# load training samples\n",
    "train_samples, eval_samples = load_tts_samples(dataset_config, eval_split=True)\n",
    "\n",
    "# init model\n",
    "model = StyleForwardTTS(config)\n",
    "\n",
    "# # init the trainer and ðŸš€\n",
    "# trainer = Trainer(\n",
    "#     TrainingArgs(),\n",
    "#     config,\n",
    "#     output_path,\n",
    "#     model=model,\n",
    "#     train_samples=train_samples,\n",
    "#     eval_samples=eval_samples,\n",
    "#     training_assets={\"audio_processor\": ap},\n",
    "# )\n",
    "# # Data loader\n",
    "# trainer.train_loader = trainer.get_train_dataloader(trainer.training_assets,trainer.train_samples, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StyleForwardTTS(\n",
       "  (emb): Embedding(130, 384)\n",
       "  (encoder): Encoder(\n",
       "    (encoder): FFTransformerBlock(\n",
       "      (fft_layers): ModuleList(\n",
       "        (0): FFTransformer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (conv1): Conv1d(384, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (conv2): Conv1d(1024, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): FFTransformer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (conv1): Conv1d(384, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (conv2): Conv1d(1024, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): FFTransformer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (conv1): Conv1d(384, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (conv2): Conv1d(1024, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): FFTransformer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (conv1): Conv1d(384, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (conv2): Conv1d(1024, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (4): FFTransformer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (conv1): Conv1d(384, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (conv2): Conv1d(1024, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (5): FFTransformer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (conv1): Conv1d(384, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (conv2): Conv1d(1024, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (style_encoder_layer): StyleEncoder(\n",
       "    (nl_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (proj): Linear(in_features=128, out_features=384, bias=True)\n",
       "    (layer): VAEFlowStyleEncoder(\n",
       "      (ref_encoder): ReferenceEncoder(\n",
       "        (convs): ModuleList(\n",
       "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "        (bns): ModuleList(\n",
       "          (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (recurrence): GRU(256, 128, batch_first=True)\n",
       "      )\n",
       "      (q_z_layers_pre): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=300, bias=True)\n",
       "        (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "      )\n",
       "      (q_z_layers_gate): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=300, bias=True)\n",
       "        (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "      )\n",
       "      (q_z_mean): Linear(in_features=300, out_features=128, bias=True)\n",
       "      (q_z_logvar): Linear(in_features=300, out_features=128, bias=True)\n",
       "      (v_layers): ModuleList(\n",
       "        (0): Linear(in_features=300, out_features=128, bias=True)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (5): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (7): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (8): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (9): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (10): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (11): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (12): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (13): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (14): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (15): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (sigmoid): Sigmoid()\n",
       "      (Gate): Gate()\n",
       "      (HF): HF()\n",
       "    )\n",
       "  )\n",
       "  (pos_encoder): PositionalEncoding()\n",
       "  (decoder): Decoder(\n",
       "    (decoder): FFTransformerDecoder(\n",
       "      (transformer_block): FFTransformerBlock(\n",
       "        (fft_layers): ModuleList(\n",
       "          (0): FFTransformer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (conv1): Conv1d(384, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (conv2): Conv1d(1024, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): FFTransformer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (conv1): Conv1d(384, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (conv2): Conv1d(1024, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): FFTransformer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (conv1): Conv1d(384, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (conv2): Conv1d(1024, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (3): FFTransformer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (conv1): Conv1d(384, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (conv2): Conv1d(1024, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (4): FFTransformer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (conv1): Conv1d(384, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (conv2): Conv1d(1024, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (5): FFTransformer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (conv1): Conv1d(384, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (conv2): Conv1d(1024, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (postnet): Conv1d(384, 80, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (duration_predictor): DurationPredictor(\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (conv_1): Conv1d(384, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (norm_1): LayerNorm()\n",
       "    (conv_2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (norm_2): LayerNorm()\n",
       "    (proj): Conv1d(256, 1, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (pitch_predictor): DurationPredictor(\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (conv_1): Conv1d(384, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (norm_1): LayerNorm()\n",
       "    (conv_2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (norm_2): LayerNorm()\n",
       "    (proj): Conv1d(256, 1, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (pitch_emb): Conv1d(1, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (aligner): AlignmentNetwork(\n",
       "    (softmax): Softmax(dim=3)\n",
       "    (log_softmax): LogSoftmax(dim=3)\n",
       "    (key_layer): Sequential(\n",
       "      (0): Conv1d(384, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (1): ReLU()\n",
       "      (2): Conv1d(768, 80, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (query_layer): Sequential(\n",
       "      (0): Conv1d(80, 160, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (1): ReLU()\n",
       "      (2): Conv1d(160, 80, kernel_size=(1,), stride=(1,))\n",
       "      (3): ReLU()\n",
       "      (4): Conv1d(80, 80, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAEFlow report - Using Cyclical Annealing:  True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StyleForwardTTSLoss(\n",
       "  (spec_loss): MSELossMasked()\n",
       "  (dur_loss): MSELossMasked()\n",
       "  (aligner_loss): ForwardSumLoss(\n",
       "    (log_softmax): LogSoftmax(dim=3)\n",
       "    (ctc_loss): CTCLoss()\n",
       "  )\n",
       "  (pitch_loss): MSELossMasked()\n",
       "  (ssim): SSIMLoss()\n",
       "  (criterion_se): VAEFlowStyleEncoderLoss()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_criterion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m_audio",
   "language": "python",
   "name": "m_audio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
